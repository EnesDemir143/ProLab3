{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "start = time.time()\n",
    "df = pd.read_excel(\"/Users/enesdemir/Desktop/ProLab3/Data/PROLAB 3 - DATASET.xlsx\")\n",
    "end = time.time()\n",
    "\n"
   ],
   "id": "218f178ae12491b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Undirected Graphs:Çift yönlülerdir sıra yoktur.\n",
    "Directed Graphs:Tek yönlülerdir.Sırayla giderler tek yönlü olduklarından.(u,v)=>node u to node v.\n",
    "Weighted graphs:Nodelar geçiş sırasında belli maliyetleri vardır.Directed veya Undirected olabilirler.(Uncdirected da bir edgenin her node geçiş cost'u aynıdır.)\n",
    "Tree:Özel bir graph türüdür.Directed veya Undirected olabilirler.Directed olanlara Root tree denir.\n",
    "Directed cycling graphs(DAG's):Cycles olmayan directed graphlerdir.\n",
    "Bipartite Graphs:2 grup vardır sadece U ve V gibi.Her Node 3 geçişi vardır.\n",
    "Complate Graphs:Tüm geçişlerin bulunduğu graphlardır genelde algoritma kontrollerinde kullanılırlar.(K1,K2 ... )(K1:tek başına ... K4:5 geçiş vardır.)\n",
    "Adjacency matrix:Edge weight lere göre matrislere yazılır tüm edgeler.Burdan cost bulmak daha kolaydır.Sorun büyük graphlarda çok zaman ve alan kaybına yol açar.\n",
    "Adjacency List:Graphları bir map olarak gösterir.A->[(B,4)(C,1)] şeklinde.Daha yoğun graphlarda alan zaman kaybı ve tüm edgelere erişmekte zorluk yaşar.\n",
    "Edge List:Tüm edgelerin nereye gittiğini ve costlarını tutar.[(C,A,4),(A,C,3),(A,B,5)] gibi kullanılır.Çok basit bir yapı olsa da adjacency list gibi dezavantajı vardır.\n",
    "\n",
    "NODE:Her harf bir Node dur.\n",
    "EDGE:Nodeları bağlayan yerlerdir."
   ],
   "id": "7ef5782c43ee0dcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tree ve Graphs en temel farkı tree de bir NODE dan diğer NODE a sadece tek yol vardır.",
   "id": "416a78914ae5fb65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T07:39:39.652842Z",
     "start_time": "2024-12-06T07:39:39.609367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def heapify(arr,n,i):\n",
    "    smallest = i\n",
    "    left = 2*i+1\n",
    "    right = 2*i+2\n",
    "\n",
    "    if left < n and arr[left][0] < arr[smallest][0] :\n",
    "        smallest = left\n",
    "\n",
    "    if right < n and arr[right][0]  < arr[smallest][0] :\n",
    "        smallest = right\n",
    "\n",
    "    if smallest != i:\n",
    "        arr[smallest], arr[i] = arr[i], arr[smallest]\n",
    "\n",
    "        heapify(arr,n,smallest)\n",
    "\n",
    "def heapSort(arr,i,n):\n",
    "    arr[i],arr[n-1]=arr[n-1],arr[i]\n",
    "\n",
    "    heapify(arr,n-1,i)\n",
    "\n",
    "def heapPop(arr,n,i):\n",
    "    if n == 0:\n",
    "        return None\n",
    "\n",
    "    arr[i],arr[n-1]=arr[n-1],arr[i]\n",
    "\n",
    "    heapify(arr,n-1,i)\n",
    "    return arr.pop()\n",
    "\n",
    "def heapPush(arr,n,val):\n",
    "    arr.append(val)\n",
    "\n",
    "    while n>0:\n",
    "        parent = (n - 1) // 2\n",
    "        if arr[n][0] < arr[parent][0]:\n",
    "            arr[parent], arr[n] = arr[n], arr[parent]\n",
    "            n = parent\n",
    "        else:\n",
    "            break\n"
   ],
   "id": "fbbda95c7898fd7e",
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 2, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mParserError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpolars\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpl\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPROLAB 3 - DATASET(in).csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ProLab3/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ProLab3/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[0;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ProLab3/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[1;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[1;32m   1919\u001B[0m     (\n\u001B[1;32m   1920\u001B[0m         index,\n\u001B[1;32m   1921\u001B[0m         columns,\n\u001B[1;32m   1922\u001B[0m         col_dict,\n\u001B[0;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[1;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[1;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/Desktop/ProLab3/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[0;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[1;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[0;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:905\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:874\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:891\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mparsers.pyx:2061\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mParserError\u001B[0m: Error tokenizing data. C error: Expected 1 fields in line 2, saw 2\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dijkstra(graph, src, dest):\n",
    "    history=[]\n",
    "    import  sys\n",
    "    inf=sys.maxsize\n",
    "    node_Data={}\n",
    "    for node in graph:\n",
    "        node_Data[node]= {\n",
    "            \"cost\" :inf,\n",
    "            \"path\":[]\n",
    "        }\n",
    "\n",
    "    node_Data[src][\"cost\"]=0\n",
    "    node_Data[src][\"path\"].append(src)\n",
    "\n",
    "    min_heap=[]\n",
    "    heapPush(min_heap,0,(0,src))\n",
    "\n",
    "    visited_Nodes=set()\n",
    "\n",
    "    while min_heap:\n",
    "        history.append({k: {\"cost\": v[\"cost\"], \"path\": v[\"path\"].copy()} for k, v in node_Data.items()})\n",
    "        current_cost, temp = min_heap[0]\n",
    "        heapPop(min_heap, len(min_heap), 0)\n",
    "\n",
    "        if temp == dest:\n",
    "            return str(node_Data[dest][\"cost\"]), node_Data[dest][\"path\"],history\n",
    "\n",
    "        if temp in visited_Nodes:\n",
    "            continue\n",
    "\n",
    "\n",
    "        visited_Nodes.add(temp)\n",
    "        for j in graph[temp]:\n",
    "            if  j not in visited_Nodes:\n",
    "                cost=node_Data[temp][\"cost\"]+graph[temp][j]\n",
    "                if cost < node_Data[j][\"cost\"]:\n",
    "                    node_Data[j][\"cost\"]=cost\n",
    "                    node_Data[j][\"path\"]=node_Data[temp][\"path\"] + [j]\n",
    "                    heapPush(min_heap,len(min_heap),(cost,j))\n",
    "\n",
    "\n",
    "    return \"Yol yok\", [],history\n",
    "\n",
    "\n",
    "graph = {\n",
    "    'A': {'B': 4, 'C': 2},\n",
    "    'B': {'A': 4, 'C': 1, 'D': 5},\n",
    "    'C': {'A': 2, 'B': 1, 'D': 8, 'E': 10},\n",
    "    'D': {'B': 5, 'C': 8, 'E': 2},\n",
    "    'E': {'C': 10, 'D': 2}\n",
    "}\n",
    "start=time.time()\n",
    "maliyet, yol ,history= dijkstra(graph, 'A', 'E')\n",
    "end=time.time()\n",
    "for i, step in enumerate(history, 1):\n",
    "    print(f\"\\nAdım {i}:\")\n",
    "    for node, data in step.items():\n",
    "        print(f\"{node}: Maliyet = {data['cost']}, Yol = {data['path']}\")\n",
    "print(f\"En kısa yol maliyeti: {maliyet}\")\n",
    "print(f\"Yol: {' -> '.join(yol)}\")\n",
    "print(end-start)"
   ],
   "id": "b041692e4b49cf6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Author:\n",
    "    def __init__(self,name, id=0):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.article = set()\n"
   ],
   "id": "b6036eb917961754"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ids = set()\n",
    "    authorsObjects = []\n",
    "    authorsObjectsNames = []\n",
    "    articlesDict = {}"
   ],
   "id": "d988c28a2cc7a3af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    for dois, paper in zip(df[\"doi\"], df[\"paper_title\"]):\n",
    "        articlesDict[dois] = paper\n",
    "    print(len(articlesDict))\n",
    "    print(len(df[\"doi\"]))\n"
   ],
   "id": "2cc4407328c8c815"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    author_to_id = {}\n",
    "    author_to_name = {}\n",
    "    for id, name in zip(df[\"orcid\"], df[\"author_name\"]):\n",
    "        name =name.strip()\n",
    "        if name in \" A. Parandaman\":\n",
    "            print(len(name))\n",
    "        if id not in ids:\n",
    "            ids.add(id)\n",
    "            author = Author(id,name)\n",
    "            authorsObjects.append(author)\n",
    "            author_to_id[id] = author\n",
    "            author_to_name[name] = author\n",
    "        elif id in ids:\n",
    "            author_to_name[name]=author_to_id[id]\n",
    "    print(len(author_to_name))\n",
    "    print(len(authorsObjects))"
   ],
   "id": "98e91d7163ce9b36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    start = time.time()\n",
    "    print(\"Makaleler işleniyor...\")\n",
    "\n",
    "    # Her makale için yazarları işle\n",
    "    for coauthors_str, doi, index in zip(df[\"coauthors\"], df[\"doi\"],df[\"author_position\"]):\n",
    "        coauthors=coauthors_str.strip('[]').replace(\"'\", \"\").split(',')\n",
    "        coauthors=[name.strip() for name in coauthors]\n",
    "\n",
    "        author_to_name[coauthors[index-1]].article.add(doi)\n",
    "\n",
    "    print(author_to_id[\"0000-0003-0788-1499\"].article)\n"
   ],
   "id": "fafd12a8e4c2c9a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "37a49ea533329dee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
